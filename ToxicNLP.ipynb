{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Toxic Comment Detection (Jupyter Notebook Edition)\n\nThis notebook reproduces the full toxic comment detection workflow\u2014data loading, preprocessing, feature extraction, model training, hyper-parameter tuning, evaluation, and inference\u2014in a single, self-contained environment. Run the notebook top-to-bottom inside Jupyter or JupyterLab to generate all artefacts and visualisations.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Environment Setup\n\nThe following cell imports every library used throughout the workflow, defines project-wide constants, and ensures that output folders exist for metrics and figures.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "import re\n",
    "from typing import Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "DATA_PATH = Path(\"train.csv\")\n",
    "ARTIFACTS_DIR = Path(\"artifacts\")\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "EXTERNAL_TEST_PATH = Path(\"test.csv\")\n",
    "EXTERNAL_RESULTS_DIR = RESULTS_DIR / \"external_test\"\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "EXTERNAL_RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load the Dataset\n\nThe Kaggle-style dataset ships with the repository as `train.csv`. It contains two columns: `comment_text` and the binary target `toxic`. We normalise the column names for convenience and perform a quick integrity check.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "df_raw = pd.read_csv(DATA_PATH)\ndf_raw.columns = [c.strip().lower() for c in df_raw.columns]\nif not {\"comment_text\", \"toxic\"}.issubset(df_raw.columns):\n    raise ValueError(\"Expected 'comment_text' and 'toxic' columns in train.csv\")\n\ndf_raw = df_raw[[\"comment_text\", \"toxic\"]].copy()\ndf_raw[\"toxic\"] = df_raw[\"toxic\"].astype(int)\nprint(df_raw.shape)\ndf_raw.head()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Text Preprocessing Utilities\n\nWe replicate the five-stage cleaning pipeline from the original project: lower-casing, punctuation removal, tokenisation, stop-word removal, and lemmatisation. Each helper function is pure so that the transformations can be chained or reused individually.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    return str(text).lower()\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z\\s]\", \"\", str(text))\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return str(text).split()\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: Iterable[str]) -> List[str]:\n",
    "    return [w for w in tokens if w and w not in STOP_WORDS]\n",
    "\n",
    "\n",
    "def lemmatise(tokens: Iterable[str]) -> List[str]:\n",
    "    return [LEMMATIZER.lemmatize(w) for w in tokens]\n",
    "\n",
    "\n",
    "def join_tokens(tokens: Iterable[str]) -> str:\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def clean_comment(text: str) -> Tuple[str, List[str]]:\n",
    "    lowered = lowercase(text)\n",
    "    no_punct = remove_punctuation(lowered)\n",
    "    tokens = tokenize(no_punct)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = lemmatise(tokens)\n",
    "    cleaned = join_tokens(tokens)\n",
    "    return cleaned, tokens\n",
    "\n",
    "\n",
    "def preprocess_comments(\n",
    "    comments: Iterable[str],\n",
    "    labels: Optional[Iterable[int]] = None,\n",
    ") -> pd.DataFrame:\n",
    "    processed_records = []\n",
    "    lemmatised_tokens = []\n",
    "    for comment in map(str, comments):\n",
    "        cleaned, tokens = clean_comment(comment)\n",
    "        processed_records.append(cleaned)\n",
    "        lemmatised_tokens.append(tokens)\n",
    "    data = {\n",
    "        \"comment\": processed_records,\n",
    "        \"tokens\": lemmatised_tokens,\n",
    "    }\n",
    "    if labels is not None:\n",
    "        data[\"toxic\"] = np.asarray(labels, dtype=int)\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Run the Preprocessing Pipeline\n\nThis step materialises intermediate artefacts that mirror the original script. Besides the final cleaned text we also persist TF\u2013IDF matrices and helper CSV files so that downstream analysis can reuse them if needed.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_comments(\n",
    "    df_raw[\"comment_text\"],\n",
    "    labels=df_raw[\"toxic\"].values,\n",
    ")\n",
    "\n",
    "preprocessed_df.to_csv(ARTIFACTS_DIR / \"final_preprocessed.csv\", index=False)\n",
    "preprocessed_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: External Test Set\n",
    "\n",
    "If a `test.csv` file is present (with at least `comment_text` and optional `id`/`toxic` columns), the next cell will preprocess it so that baseline and tuned models can be compared on the held-out data.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "external_test_df = None\n",
    "external_test_labels = None\n",
    "\n",
    "if EXTERNAL_TEST_PATH.exists():\n",
    "    external_raw = pd.read_csv(EXTERNAL_TEST_PATH)\n",
    "    external_raw.columns = [c.strip().lower() for c in external_raw.columns]\n",
    "    if \"comment_text\" not in external_raw.columns:\n",
    "        raise ValueError(\"Expected 'comment_text' column in test.csv\")\n",
    "    if \"id\" not in external_raw.columns:\n",
    "        external_raw[\"id\"] = np.arange(len(external_raw))\n",
    "    label_values = external_raw[\"toxic\"].values if \"toxic\" in external_raw.columns else None\n",
    "    external_test_df = preprocess_comments(\n",
    "        external_raw[\"comment_text\"],\n",
    "        labels=label_values,\n",
    "    )\n",
    "    external_test_df.insert(0, \"comment_text\", external_raw[\"comment_text\"].astype(str).values)\n",
    "    external_test_df.insert(0, \"id\", external_raw[\"id\"].values)\n",
    "    if \"toxic\" in external_test_df.columns:\n",
    "        external_test_labels = external_test_df[\"toxic\"].values.astype(int)\n",
    "    external_test_df.to_csv(ARTIFACTS_DIR / \"external_test_preprocessed.csv\", index=False)\n",
    "    print(f\"External test set loaded with {len(external_test_df)} rows.\")\n",
    "else:\n",
    "    print(f\"No external test set found at {EXTERNAL_TEST_PATH.resolve()}. Skipping external evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We also inspect token counts to ensure the cleaning behaves as expected.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "word_counts = preprocessed_df[\"comment\"].apply(lambda text: len(text.split()))\nplt.figure(figsize=(8, 4))\nplt.hist(word_counts, bins=50, color=\"#4C72B0\")\nplt.title(\"Word Count Distribution After Cleaning\")\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Frequency\")\nplt.tight_layout()\nplt.savefig(ARTIFACTS_DIR / \"wordcount_hist.png\", bbox_inches=\"tight\")\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. TF\u2013IDF Feature Extraction\n\nThe notebook limits the vocabulary to 5,000 terms (minimum document frequency of 2) to remain faithful to the Python package version.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, min_df=2)\n",
    "X_tfidf = vectorizer.fit_transform(preprocessed_df[\"comment\"].values)\n",
    "y = preprocessed_df[\"toxic\"].values\n",
    "\n",
    "external_test_matrix = None\n",
    "if external_test_df is not None:\n",
    "    external_test_matrix = vectorizer.transform(external_test_df[\"comment\"].values)\n",
    "    sparse.save_npz(ARTIFACTS_DIR / \"external_test_X_tfidf.npz\", external_test_matrix)\n",
    "    print(\"External test TF-IDF matrix shape:\", external_test_matrix.shape)\n",
    "\n",
    "sparse.save_npz(ARTIFACTS_DIR / \"X_tfidf.npz\", X_tfidf)\n",
    "pd.DataFrame({\"toxic\": y}).to_csv(ARTIFACTS_DIR / \"y.csv\", index=False)\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Train/Test Split\n\nWe stratify the split to preserve the 50/50 class balance.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X_tfidf,\n    y,\n    test_size=0.2,\n    random_state=RANDOM_STATE,\n    stratify=y,\n)\n\nX_train.shape, X_test.shape\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Evaluation Helpers\n\nUtility functions for metric computation, ROC handling, confusion-matrix plotting, and the reusable training loop are defined below.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_for_roc(model, X):\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return model.decision_function(X)\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_score=None):\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"roc_auc\": np.nan,\n",
    "    }\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, y_score)\n",
    "        except ValueError:\n",
    "            metrics[\"roc_auc\"] = np.nan\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def summarise_metrics(model_name, phase, metrics):\n",
    "    row = {\"model\": model_name, \"phase\": phase}\n",
    "    row.update(metrics)\n",
    "    return row\n",
    "\n",
    "\n",
    "def display_confusion_matrix(y_true, y_pred, title, ax=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    created_fig = False\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "        created_fig = True\n",
    "    disp = ConfusionMatrixDisplay(cm)\n",
    "    disp.plot(ax=ax, values_format='d', cmap='Blues', colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "    if created_fig:\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def display_roc_curve(y_true, y_score, title, ax=None):\n",
    "    created_fig = False\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "        created_fig = True\n",
    "    if y_score is None:\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5, 0.5, 'ROC curve not available', ha='center', va='center', fontsize=11)\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        ax.plot(fpr, tpr, label='Model', color='#4C72B0')\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Chance')\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "    if created_fig:\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model-Specific Training Blocks\n",
    "\n",
    "Each classifier now runs inside its own code cell so you can inspect the baseline training, cross-validation, hyper-parameter tuning, validation metrics, comparison plots, and optional external-test evaluation independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metric_order = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "CV_FOLDS = 3\n",
    "SCORING_METRIC = 'f1'\n",
    "\n",
    "all_model_results = {}\n",
    "metrics_summary_rows = []\n",
    "tuned_models = {}\n",
    "best_model_name: Optional[str] = None\n",
    "best_model_metrics: Optional[dict] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_model_workflow(name, short_name, factory, param_grid, scoring=SCORING_METRIC, cv=CV_FOLDS):\n",
    "    # Train baseline and tuned variants of a model, returning a rich results dictionary.\n",
    "    baseline_model = factory()\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    baseline_pred = baseline_model.predict(X_test)\n",
    "    baseline_score = get_scores_for_roc(baseline_model, X_test)\n",
    "    baseline_metrics = compute_metrics(y_test, baseline_pred, baseline_score)\n",
    "    cv_scores = cross_val_score(factory(), X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    grid_search = GridSearchCV(\n",
    "        factory(),\n",
    "        param_grid,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    tuned_model = grid_search.best_estimator_\n",
    "    tuned_pred = tuned_model.predict(X_test)\n",
    "    tuned_score = get_scores_for_roc(tuned_model, X_test)\n",
    "    tuned_metrics = compute_metrics(y_test, tuned_pred, tuned_score)\n",
    "    result = {\n",
    "        'name': name,\n",
    "        'short_name': short_name,\n",
    "        'baseline': {\n",
    "            'model': baseline_model,\n",
    "            'metrics': baseline_metrics,\n",
    "            'y_pred': baseline_pred,\n",
    "            'y_score': baseline_score,\n",
    "        },\n",
    "        'tuned': {\n",
    "            'model': tuned_model,\n",
    "            'metrics': tuned_metrics,\n",
    "            'y_pred': tuned_pred,\n",
    "            'y_score': tuned_score,\n",
    "            'best_params': grid_search.best_params_,\n",
    "        },\n",
    "        'cv_scores': cv_scores,\n",
    "        'grid_search': grid_search,\n",
    "    }\n",
    "    metrics_summary_rows.append(summarise_metrics(name, 'baseline', baseline_metrics))\n",
    "    metrics_summary_rows.append(summarise_metrics(name, 'tuned', tuned_metrics))\n",
    "    tuned_models[name] = tuned_model\n",
    "    if external_test_matrix is not None:\n",
    "        baseline_external_pred = baseline_model.predict(external_test_matrix)\n",
    "        tuned_external_pred = tuned_model.predict(external_test_matrix)\n",
    "        baseline_external_score = get_scores_for_roc(baseline_model, external_test_matrix)\n",
    "        tuned_external_score = get_scores_for_roc(tuned_model, external_test_matrix)\n",
    "        external_bundle = {\n",
    "            'baseline': {\n",
    "                'pred': baseline_external_pred,\n",
    "                'score': baseline_external_score,\n",
    "            },\n",
    "            'tuned': {\n",
    "                'pred': tuned_external_pred,\n",
    "                'score': tuned_external_score,\n",
    "            },\n",
    "        }\n",
    "        if external_test_labels is not None:\n",
    "            external_bundle['baseline']['metrics'] = compute_metrics(\n",
    "                external_test_labels, baseline_external_pred, baseline_external_score\n",
    "            )\n",
    "            external_bundle['tuned']['metrics'] = compute_metrics(\n",
    "                external_test_labels, tuned_external_pred, tuned_external_score\n",
    "            )\n",
    "        output_columns = {\n",
    "            'id': external_test_df['id'],\n",
    "            'comment_text': external_test_df['comment_text'],\n",
    "            'clean_comment': external_test_df['comment'],\n",
    "            'baseline_prediction': baseline_external_pred,\n",
    "            'tuned_prediction': tuned_external_pred,\n",
    "        }\n",
    "        if baseline_external_score is not None:\n",
    "            output_columns['baseline_score'] = baseline_external_score\n",
    "        if tuned_external_score is not None:\n",
    "            output_columns['tuned_score'] = tuned_external_score\n",
    "        predictions_df = pd.DataFrame(output_columns)\n",
    "        predictions_path = EXTERNAL_RESULTS_DIR / f\"{short_name}_predictions.csv\"\n",
    "        predictions_df.to_csv(predictions_path, index=False)\n",
    "        external_bundle['predictions_path'] = predictions_path\n",
    "        result['external'] = external_bundle\n",
    "    all_model_results[name] = result\n",
    "    return result\n",
    "def present_model_results(result):\n",
    "    model_name = result['name']\n",
    "    cv_scores = result['cv_scores']\n",
    "    display(Markdown(f\"**{CV_FOLDS}-fold cross-validation ({SCORING_METRIC.upper()})**\"))\n",
    "    print(f\"Scores: {np.round(cv_scores, 4)}\")\n",
    "    print(f\"Mean: {np.mean(cv_scores):.4f} \u00b1 {np.std(cv_scores):.4f}\")\n",
    "    display(Markdown(\"**Best hyper-parameters from tuning**\"))\n",
    "    print(result['tuned']['best_params'])\n",
    "    cv_results_df = pd.DataFrame(result['grid_search'].cv_results_)\n",
    "    display(\n",
    "        cv_results_df[\n",
    "            ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "        ].sort_values('rank_test_score').head()\n",
    "    )\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Baseline': result['baseline']['metrics'],\n",
    "        'Tuned': result['tuned']['metrics'],\n",
    "    }).loc[metric_order]\n",
    "    display(Markdown(\"**Validation set metrics**\"))\n",
    "    display(comparison_df)\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    comparison_df.T.plot(kind='bar', ax=ax, color=['#4C72B0', '#55A868'])\n",
    "    ax.set_title(f\"{model_name} \u2013 Baseline vs Tuned Metrics\")\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.xticks(rotation=45)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    display_confusion_matrix(y_test, result['baseline']['y_pred'], f\"{model_name} \u2013 Baseline\", ax=axes[0])\n",
    "    display_confusion_matrix(y_test, result['tuned']['y_pred'], f\"{model_name} \u2013 Tuned\", ax=axes[1])\n",
    "    fig.suptitle(f\"{model_name} \u2013 Confusion Matrices\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    display_roc_curve(y_test, result['baseline']['y_score'], f\"{model_name} \u2013 Baseline ROC\", ax=axes[0])\n",
    "    display_roc_curve(y_test, result['tuned']['y_score'], f\"{model_name} \u2013 Tuned ROC\", ax=axes[1])\n",
    "    fig.suptitle(f\"{model_name} \u2013 ROC Curves\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "    external = result.get('external')\n",
    "    if external is None:\n",
    "        print('External test evaluation skipped (no test.csv detected).')\n",
    "    else:\n",
    "        baseline_metrics = external['baseline'].get('metrics')\n",
    "        tuned_metrics = external['tuned'].get('metrics')\n",
    "        if baseline_metrics and tuned_metrics:\n",
    "            external_df = pd.DataFrame({\n",
    "                'Baseline': baseline_metrics,\n",
    "                'Tuned': tuned_metrics,\n",
    "            }).loc[metric_order]\n",
    "            display(Markdown(\"**External test metrics**\"))\n",
    "            display(external_df)\n",
    "        else:\n",
    "            print('External test set does not provide labels; predictions saved to:')\n",
    "            print(str(external['predictions_path']))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "logreg_results = run_model_workflow(\n",
    "    name='Logistic Regression',\n",
    "    short_name='logreg',\n",
    "    factory=lambda: LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    param_grid={\n",
    "        'C': [0.1, 1.0, 5.0],\n",
    "        'solver': ['liblinear', 'lbfgs'],\n",
    "        'penalty': ['l2'],\n",
    "    },\n",
    ")\n",
    "present_model_results(logreg_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "nb_results = run_model_workflow(\n",
    "    name='Multinomial Naive Bayes',\n",
    "    short_name='nb',\n",
    "    factory=lambda: MultinomialNB(),\n",
    "    param_grid={\n",
    "        'alpha': [0.1, 1.0, 10.0],\n",
    "        'fit_prior': [True, False],\n",
    "    },\n",
    ")\n",
    "present_model_results(nb_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_results = run_model_workflow(\n",
    "    name='Linear SVM (LinearSVC)',\n",
    "    short_name='svm',\n",
    "    factory=lambda: LinearSVC(random_state=RANDOM_STATE, dual=False),\n",
    "    param_grid={\n",
    "        'C': [0.5, 1.0, 2.0],\n",
    "        'loss': ['squared_hinge'],\n",
    "        'class_weight': [None, 'balanced'],\n",
    "    },\n",
    ")\n",
    "present_model_results(svm_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "dt_results = run_model_workflow(\n",
    "    name='Decision Tree',\n",
    "    short_name='dt',\n",
    "    factory=lambda: DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    param_grid={\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [None, 20, 40],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 5],\n",
    "    },\n",
    ")\n",
    "present_model_results(dt_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf_results = run_model_workflow(\n",
    "    name='Random Forest',\n",
    "    short_name='rf',\n",
    "    factory=lambda: RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=200),\n",
    "    param_grid={\n",
    "        'n_estimators': [200, 400],\n",
    "        'max_depth': [None, 20, 40],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 5],\n",
    "    },\n",
    ")\n",
    "present_model_results(rf_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_results = run_model_workflow(\n",
    "    name='k-Nearest Neighbours',\n",
    "    short_name='knn',\n",
    "    factory=lambda: KNeighborsClassifier(),\n",
    "    param_grid={\n",
    "        'n_neighbors': [3, 5, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['minkowski', 'manhattan'],\n",
    "    },\n",
    ")\n",
    "present_model_results(knn_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Consolidated Metrics and Model Ranking\n",
    "\n",
    "The table below aggregates validation metrics for every baseline and tuned model. It also identifies the best-performing tuned estimator by F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics_summary_rows)\n",
    "if metrics_df.empty:\n",
    "    raise RuntimeError('No metrics captured\u2014ensure each model cell executed successfully.')\n",
    "metrics_df = metrics_df.sort_values(['model', 'phase']).reset_index(drop=True)\n",
    "metrics_df.to_csv(RESULTS_DIR / 'model_metrics.csv', index=False)\n",
    "display(metrics_df)\n",
    "\n",
    "tuned_only = metrics_df[metrics_df['phase'] == 'tuned'].copy()\n",
    "if tuned_only.empty:\n",
    "    raise RuntimeError('Tuned metrics missing.')\n",
    "best_row = tuned_only.sort_values('f1', ascending=False).iloc[0]\n",
    "best_model_name = best_row['model']\n",
    "best_model_metrics = best_row.to_dict()\n",
    "display(Markdown(\n",
    "    f\"**Best tuned model:** {best_model_name} (F1 = {best_row['f1']:.4f}, Accuracy = {best_row['accuracy']:.4f}, \"\n",
    "    f\"Precision = {best_row['precision']:.4f}, Recall = {best_row['recall']:.4f}, AUC = {best_row['roc_auc']:.4f})\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "comparison_table = metrics_df.pivot_table(index='model', columns='phase', values=metric_order)\n",
    "comparison_table = comparison_table.reindex(columns=['baseline', 'tuned'], level=1)\n",
    "comparison_table = comparison_table.sort_index()\n",
    "display(comparison_table)\n",
    "\n",
    "fig, axes = plt.subplots(len(metric_order), 1, figsize=(10, 3 * len(metric_order)))\n",
    "for idx, metric in enumerate(metric_order):\n",
    "    ax = axes[idx]\n",
    "    subset = metrics_df.pivot(index='model', columns='phase', values=metric).reindex(comparison_table.index)\n",
    "    subset.plot(kind='bar', ax=ax, color=['#4C72B0', '#55A868'])\n",
    "    ax.set_title(f'{metric.title()} by Model (Baseline vs Tuned)')\n",
    "    ax.set_ylabel(metric.title())\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_xlabel('')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. External Test Evaluation Summary\n",
    "\n",
    "When `test.csv` is available, predictions and metrics (if labels are present) are collected below for a side-by-side comparison of baseline versus tuned estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if external_test_matrix is not None:\n",
    "    summary_rows = []\n",
    "    for name, result in all_model_results.items():\n",
    "        external = result.get('external')\n",
    "        if not external:\n",
    "            continue\n",
    "        baseline_pred = external['baseline']['pred']\n",
    "        tuned_pred = external['tuned']['pred']\n",
    "        row = {\n",
    "            'model': name,\n",
    "            'baseline_positive_rate': float(np.mean(baseline_pred)),\n",
    "            'tuned_positive_rate': float(np.mean(tuned_pred)),\n",
    "            'prediction_shift': float(np.mean(tuned_pred) - np.mean(baseline_pred)),\n",
    "            'prediction_agreement': float(np.mean(baseline_pred == tuned_pred)),\n",
    "            'predictions_path': str(external.get('predictions_path', '')),\n",
    "        }\n",
    "        baseline_metrics = external['baseline'].get('metrics')\n",
    "        tuned_metrics = external['tuned'].get('metrics')\n",
    "        if baseline_metrics and tuned_metrics:\n",
    "            for metric in metric_order:\n",
    "                row[f'{metric}_baseline'] = baseline_metrics[metric]\n",
    "                row[f'{metric}_tuned'] = tuned_metrics[metric]\n",
    "        summary_rows.append(row)\n",
    "    if summary_rows:\n",
    "        external_summary_df = pd.DataFrame(summary_rows)\n",
    "        external_summary_df.to_csv(EXTERNAL_RESULTS_DIR / 'summary.csv', index=False)\n",
    "        display(external_summary_df)\n",
    "        print(f'External predictions written to {EXTERNAL_RESULTS_DIR.resolve()}')\n",
    "    else:\n",
    "        print('External test set detected but no predictions recorded.')\n",
    "else:\n",
    "    print(f'External test evaluation skipped (no test.csv at {EXTERNAL_TEST_PATH.resolve()}).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Inference Helper\n",
    "\n",
    "Select the best tuned estimator (highest F1 score) or experiment with any trained model to classify arbitrary comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers for single-comment inference across tuned models\n",
    "def predict_single_comment(model_name: str, comment: str) -> Tuple[int, Optional[float]]:\n",
    "    if not tuned_models:\n",
    "        raise RuntimeError('No trained tuned models detected. Run the training cells first.')\n",
    "    if model_name not in tuned_models:\n",
    "        raise ValueError(f'Unknown model: {model_name}')\n",
    "    if not comment.strip():\n",
    "        raise ValueError('Comment must not be empty.')\n",
    "    cleaned, _ = clean_comment(comment)\n",
    "    features = vectorizer.transform([cleaned])\n",
    "    model = tuned_models[model_name]\n",
    "    prediction = int(model.predict(features)[0])\n",
    "    confidence: Optional[float] = None\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        confidence = float(model.predict_proba(features)[0, 1])\n",
    "    elif hasattr(model, 'decision_function'):\n",
    "        confidence = float(model.decision_function(features)[0])\n",
    "    return prediction, confidence\n",
    "\n",
    "model_names = sorted(tuned_models.keys())\n",
    "if not model_names:\n",
    "    raise RuntimeError('No trained tuned models detected. Run the training cells first.')\n",
    "\n",
    "default_model = best_model_name if best_model_name in tuned_models else model_names[0]\n",
    "\n",
    "print('Available tuned models:')\n",
    "for idx, name in enumerate(model_names, start=1):\n",
    "    marker = ' (default)' if name == default_model else ''\n",
    "    print(f'  {idx}. {name}{marker}')\n",
    "\n",
    "try:\n",
    "    selection = input(f'Select a model by number or name [{default_model}]: ').strip()\n",
    "    if not selection:\n",
    "        model_name = default_model\n",
    "    elif selection.isdigit():\n",
    "        selection_index = int(selection)\n",
    "        if not 1 <= selection_index <= len(model_names):\n",
    "            raise ValueError('Selection index out of range.')\n",
    "        model_name = model_names[selection_index - 1]\n",
    "    else:\n",
    "        if selection not in tuned_models:\n",
    "            raise ValueError('Unknown model selection.')\n",
    "        model_name = selection\n",
    "\n",
    "    comment = input('Enter a comment to classify: ').strip()\n",
    "    prediction, confidence = predict_single_comment(model_name, comment)\n",
    "except Exception as exc:\n",
    "    print(f'Error: {exc}')\n",
    "else:\n",
    "    label = 'Toxic' if prediction else 'Not toxic'\n",
    "    print(f'Prediction ({model_name}): {label}')\n",
    "    if confidence is not None:\n",
    "        print(f'Confidence score: {confidence:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook remains intentionally linear: rerun it from the top whenever you adjust preprocessing, parameter grids, or add a new external test file to keep every artefact in sync.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}